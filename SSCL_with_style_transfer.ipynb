{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Self-Supervised Contrastive Learning with Style Transfer\n",
        "\n",
        "**Authors:** Ömer Faruk Erdem, Anıl Dervişoğlu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import necessary libraries and initializations for Style Transfer Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FNbDi8vDe9i"
      },
      "outputs": [],
      "source": [
        "# All the libraries necessary for Style Transfer Code\n",
        "import tensorflow as tf\n",
        "import keras.preprocessing.image as process_im\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications import vgg19\n",
        "from keras.models import Model\n",
        "from tensorflow.python.keras import models, losses, layers\n",
        "from tensorflow.python.keras import backend as K\n",
        "import functools\n",
        "import IPython.display\n",
        "from torchvision import transforms\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeQalnYFGRln"
      },
      "outputs": [],
      "source": [
        "# To load the file in the numpy array format given its path\n",
        "def load_file(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    max_dim = 512\n",
        "    factor = max_dim / max(image.size)\n",
        "    image = image.resize((round(image.size[0] * factor), round(image.size[1] * factor)), Image.LANCZOS)\n",
        "    im_array = process_im.img_to_array(image)\n",
        "    im_array = np.expand_dims(im_array, axis=0)  # adding extra axis to the array as to generate a batch of single image\n",
        "    return im_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3lE3fZvsbNT",
        "outputId": "834ad10e-195d-4483-f3d2-b2d0c6e1d159"
      },
      "outputs": [],
      "source": [
        "# To apply vgg19 model we preprocess the image with the given function\n",
        "def img_preprocess(img_path):\n",
        "    image=load_file(img_path)\n",
        "    img=tf.keras.applications.vgg19.preprocess_input(image)\n",
        "    return img\n",
        "\n",
        "# This part is to reverse the sretps in the preprocessing steps,\n",
        "# There are also some predefined variables in our code to calculate accurately\n",
        "def deprocess_img(processed_img):\n",
        "  x = processed_img.copy()\n",
        "  if len(x.shape) == 4:\n",
        "    x = np.squeeze(x, 0)\n",
        "\n",
        "  # performing the inverse of the preprocessing step\n",
        "  x[:, :, 0] += 103.939\n",
        "  x[:, :, 1] += 116.779\n",
        "  x[:, :, 2] += 123.68\n",
        "  x = x[:, :, ::-1] # converting BGR to RGB channel\n",
        "\n",
        "  x = np.clip(x, 0, 255).astype('uint8')\n",
        "  return x\n",
        "\n",
        "# The layers to be applied\n",
        "content_layers = ['block5_conv2']\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1',\n",
        "                'block4_conv1',\n",
        "                'block5_conv1']\n",
        "number_content = len(content_layers)\n",
        "number_style =len(style_layers)\n",
        "\n",
        "# The function to get the model ready, also the layers are concatanated here to create the model\n",
        "def get_model():\n",
        "    vgg=tf.keras.applications.vgg19.VGG19(include_top = False, weights = 'imagenet')\n",
        "    vgg.trainable = False\n",
        "    content_output = [vgg.get_layer(layer).output for layer in content_layers]\n",
        "    style_output = [vgg.get_layer(layer).output for layer in style_layers]\n",
        "    model_output = style_output + content_output\n",
        "    return models.Model(vgg.input, model_output)\n",
        "\n",
        "model=tf.keras.applications.vgg19.VGG19(include_top = False, weights = 'imagenet')\n",
        "model.summary()\n",
        "\n",
        "model = get_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMp1kUClslIa",
        "outputId": "6b806865-6697-4806-98a7-19088ab52298"
      },
      "outputs": [],
      "source": [
        "# The 3 functions below is for calculating the loss for content and style\n",
        "def get_content_loss(noise, target):\n",
        "    loss = tf.reduce_mean(tf.square(noise - target))\n",
        "    return loss\n",
        "\n",
        "def gram_matrix(tensor):\n",
        "    channels = int(tensor.shape[-1])\n",
        "    vector=tf.reshape(tensor, [-1,channels])\n",
        "    n=tf.shape(vector)[0]\n",
        "    gram_matrix=tf.matmul(vector, vector, transpose_a = True)\n",
        "    return gram_matrix / tf.cast(n,tf.float32)\n",
        "\n",
        "def get_style_loss(noise, target):\n",
        "    gram_noise=gram_matrix(noise)\n",
        "    loss = tf.reduce_mean(tf.square(target - gram_noise))\n",
        "    return loss\n",
        "\n",
        "# Per our combinations of style transfer and self-supervised contrastive learning\n",
        "# We were forced to take content_path not as a string to path but a torch tensor,\n",
        "# Thus, we have created the function below to process it fitting to the rest of the code\n",
        "def img_preprocess_v2(img):\n",
        "    image = np.array(img)\n",
        "    preprocessed_img = tf.keras.applications.vgg19.preprocess_input(image)\n",
        "    preprocessed_img = np.expand_dims(preprocessed_img, axis=0)\n",
        "    return preprocessed_img\n",
        "\n",
        "# Using the preprocessing and model functions above, we get the features\n",
        "def get_features(model,content_path, style_path):\n",
        "    content_img = img_preprocess_v2(content_path)\n",
        "    style_image = img_preprocess(style_path)\n",
        "\n",
        "    content_output = model(content_img)\n",
        "    style_output = model(style_image)\n",
        "\n",
        "    content_feature = [layer[0] for layer in content_output[number_style:]]\n",
        "    style_feature = [layer[0] for layer in style_output[:number_style]]\n",
        "    return content_feature, style_feature\n",
        "\n",
        "def compute_loss(model, loss_weights,image, gram_style_features, content_features):\n",
        "    # defining what percentage of content and/or style will be preserved in the generated image\n",
        "    style_weight, content_weight = loss_weights\n",
        "\n",
        "    output = model(image)\n",
        "    content_loss = 0\n",
        "    style_loss = 0\n",
        "\n",
        "    noise_style_features = output[:number_style]\n",
        "    noise_content_feature = output[number_style:]\n",
        "\n",
        "    weight_per_layer = 1.0/float(number_style)\n",
        "    for a, b in zip(gram_style_features, noise_style_features):\n",
        "        style_loss += weight_per_layer * get_style_loss(b[0], a)\n",
        "\n",
        "    weight_per_layer = 1.0/ float(number_content)\n",
        "    for a, b in zip(noise_content_feature, content_features):\n",
        "        content_loss += weight_per_layer * get_content_loss(a[0], b)\n",
        "\n",
        "    style_loss *= style_weight\n",
        "    content_loss *= content_weight\n",
        "\n",
        "    total_loss = content_loss + style_loss\n",
        "\n",
        "    return total_loss,style_loss, content_loss\n",
        "\n",
        "def compute_grads(dictionary):\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_loss = compute_loss(**dictionary)\n",
        "\n",
        "    total_loss = all_loss[0]\n",
        "    return tape.gradient(total_loss,dictionary['image']), all_loss\n",
        "\n",
        "model = tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')\n",
        "model.summary()\n",
        "\n",
        "# The base function which transforms the content with the corresponding style image given the weigths for each\n",
        "# count = 0\n",
        "def run_style_transfer(content_path, style_path, epochs=20, content_weight=1e4, style_weight=1e-2):\n",
        "    # global count\n",
        "    # count += 1\n",
        "    # print(f\"image {count}\")\n",
        "    model=get_model()\n",
        "\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    content_feature,style_feature = get_features(model,content_path,style_path)\n",
        "    style_gram_matrix=[gram_matrix(feature) for feature in style_feature]\n",
        "\n",
        "    noise = img_preprocess_v2(content_path)\n",
        "    noise=tf.Variable(noise,dtype=tf.float32)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n",
        "\n",
        "    best_loss,best_img=float('inf'),None\n",
        "\n",
        "    loss_weights = (style_weight, content_weight)\n",
        "    dictionary={'model':model,\n",
        "              'loss_weights':loss_weights,\n",
        "              'image':noise,\n",
        "              'gram_style_features':style_gram_matrix,\n",
        "              'content_features':content_feature}\n",
        "\n",
        "    norm_means = np.array([103.939, 116.779, 123.68])\n",
        "    min_vals = -norm_means\n",
        "    max_vals = 255 - norm_means\n",
        "\n",
        "    for i in range(epochs):\n",
        "        grad,all_loss=compute_grads(dictionary)\n",
        "        total_loss,style_loss,content_loss=all_loss\n",
        "        optimizer.apply_gradients([(grad,noise)])\n",
        "        clipped=tf.clip_by_value(noise,min_vals,max_vals)\n",
        "        noise.assign(clipped)\n",
        "\n",
        "        if total_loss<best_loss:\n",
        "            best_img = deprocess_img(noise.numpy())\n",
        "\n",
        "    img_pil = Image.fromarray((best_img * 1).astype('uint8'))\n",
        "\n",
        "    to_tensor = transforms.ToTensor()\n",
        "\n",
        "    img_tensor = to_tensor(img_pil)\n",
        "\n",
        "    return img_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import necessary libraries and initializations for SSCL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "O-na8eqY_8RN",
        "outputId": "3bdffceb-c28c-4c5e-d857-8ff34d49d5a6"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import STL10\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "\n",
        "# Path to the folder where the datasets are and should be downloaded\n",
        "DATASET_PATH = \"../data\"\n",
        "CHECKPOINT_PATH = \"../checkpoint\"\n",
        "\n",
        "# Initially, it was planned to use dataloading to decrease the heavy load, but it didn't work no matter what\n",
        "# Still, it can be tried on different machines as well\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "print(\"Number of workers:\", NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SimCLR\n",
        "\n",
        "We will start our exploration of contrastive learning by discussing the effect of different data augmentation techniques, and how we can implement an efficient data loader for such. Next, we implement SimCLR with PyTorch Lightning, and finally train it on a large, unlabeled dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation for Contrastive Learning\n",
        "\n",
        "To allow efficient training, we need to prepare the data loading such that we sample two different, random augmentations for each image in the batch. The easiest way to do this is by creating a transformation that, when being called, applies a set of data augmentations to an image twice. This is implemented in the class `ContrastiveTransformations` below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s3O1UzX_8RR"
      },
      "outputs": [],
      "source": [
        "# To create callable transformations, we had to create a class\n",
        "# Here we get the image and create two different style transfer images as its transformations\n",
        "class ContrastiveTransformations(object):\n",
        "    def __init__(self, base_transforms, n_views = 2):\n",
        "        self.base_transforms = base_transforms\n",
        "        self.n_views = n_views\n",
        "        self.contents = [\"/content/style_1.jpg\", \"/content/style_3.jpg\"]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return [self.base_transforms(x, self.contents[i], epochs = 100) for i in range(self.n_views)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d_BnyQX_8RS",
        "outputId": "51f5bebf-dfce-488c-f0fb-f5d3035fd9db"
      },
      "outputs": [],
      "source": [
        "# Downloading the data from\n",
        "unlabeled_data = STL10(root=DATASET_PATH, split='unlabeled', download=True,\n",
        "                       transform=ContrastiveTransformations(run_style_transfer, n_views=2))\n",
        "train_data_contrast = STL10(root=DATASET_PATH, split='train', download=True,\n",
        "                            transform=ContrastiveTransformations(run_style_transfer, n_views=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "a6fF8Fwn_8RS",
        "outputId": "5e4dd2b3-bb82-4f6a-cbe6-7070025a7b95"
      },
      "outputs": [],
      "source": [
        "# Visualizing some examples\n",
        "pl.seed_everything(42)\n",
        "NUM_IMAGES = 6\n",
        "count = 0\n",
        "imgs = torch.stack([img for idx in range(NUM_IMAGES) for img in unlabeled_data[idx][0]], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(imgs, nrow=6, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Augmented image examples of the STL10 dataset')\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OmyISJC_8RT"
      },
      "source": [
        "### SimCLR implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgsXMyvC_8RT"
      },
      "outputs": [],
      "source": [
        "class SimCLR(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=500):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
        "\n",
        "        self.convnet = torchvision.models.resnet18(num_classes=4*hidden_dim)  \n",
        "\n",
        "        self.convnet.fc = nn.Sequential(\n",
        "            self.convnet.fc,  # Linear(ResNet output, 4*hidden_dim)\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4*hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(),\n",
        "                                lr=self.hparams.lr,\n",
        "                                weight_decay=self.hparams.weight_decay)\n",
        "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                            T_max=self.hparams.max_epochs,\n",
        "                                                            eta_min=self.hparams.lr/50)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def info_nce_loss(self, batch, mode='train'):\n",
        "        imgs, _ = batch\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "\n",
        "        feats = self.convnet(imgs)\n",
        "\n",
        "        cos_sim = F.cosine_similarity(feats[:,None,:], feats[None,:,:], dim=-1)\n",
        "\n",
        "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
        "        cos_sim.masked_fill_(self_mask, -9e15)\n",
        "\n",
        "\n",
        "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0]//2, dims=0)\n",
        "\n",
        "\n",
        "        cos_sim = cos_sim / self.hparams.temperature\n",
        "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
        "        nll = nll.mean()\n",
        "\n",
        "\n",
        "        self.log(mode+'_loss', nll)\n",
        "        \n",
        "        \n",
        "        comb_sim = torch.cat([cos_sim[pos_mask][:,None],  \n",
        "                              cos_sim.masked_fill(pos_mask, -9e15)],\n",
        "                             dim=-1)\n",
        "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
        "        \n",
        "\n",
        "        self.log(mode+'_acc_top1', (sim_argsort == 0).float().mean())\n",
        "        self.log(mode+'_acc_top5', (sim_argsort < 5).float().mean())\n",
        "        self.log(mode+'_acc_mean_pos', 1+sim_argsort.float().mean())\n",
        "\n",
        "        return nll\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.info_nce_loss(batch, mode='train')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self.info_nce_loss(batch, mode='val')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBkmXqjU_8RT"
      },
      "source": [
        "### Training\n",
        "\n",
        "Now that we have implemented SimCLR and the data loading pipeline, we are ready to train the model. We will use the same training function setup as usual. For saving the best model checkpoint, we track the metric `val_acc_top5`, which describes how often the correct image patch is within the top-5 most similar examples in the batch. This is usually less noisy than the top-1 metric, making it a better metric to choose the best model from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QcVedhO_8RU"
      },
      "outputs": [],
      "source": [
        "def train_simclr(batch_size, max_epochs=500, **kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, 'SimCLR'),\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=max_epochs,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc_top5'),\n",
        "                                    LearningRateMonitor('epoch')])\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    train_loader = data.DataLoader(unlabeled_data, batch_size=batch_size, shuffle=True,\n",
        "                                    drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "    val_loader = data.DataLoader(train_data_contrast, batch_size=batch_size, shuffle=False,\n",
        "                                  drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "    pl.seed_everything(42) # To be reproducable\n",
        "    model = SimCLR(max_epochs=max_epochs, **kwargs)\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    model = SimCLR.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5gJidGT_8RU"
      },
      "source": [
        "A common observation in contrastive learning is that the larger the batch size, the better the models perform. A larger batch size allows us to compare each image to more negative examples, leading to overall smoother loss gradients. However, in our case, we experienced that a batch size of 256 was sufficient to get good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N6MVRa5_8RU",
        "outputId": "0988af97-f84d-4a98-a18f-46fe0119eb6a"
      },
      "outputs": [],
      "source": [
        "simclr_model = train_simclr(batch_size=256,\n",
        "                            hidden_dim=128,\n",
        "                            lr=5e-4,\n",
        "                            temperature=0.07,\n",
        "                            weight_decay=1e-4,\n",
        "                            max_epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odj9DagM_8RV"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "First, let's implement a simple Logistic Regression setup for which we assume that the images already have been encoded in their feature vectors. If very little data is available, it might be beneficial to dynamically encode the images during training so that we can also apply data augmentations. However, the way we implement it here is much more efficient and can be trained within a few seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S06NEjBt_8RV"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, feature_dim, num_classes, lr, weight_decay, max_epochs=100):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        # Mapping from representation h to classes\n",
        "        self.model = nn.Linear(feature_dim, num_classes)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(),\n",
        "                                lr=self.hparams.lr,\n",
        "                                weight_decay=self.hparams.weight_decay)\n",
        "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                      milestones=[int(self.hparams.max_epochs*0.6),\n",
        "                                                                  int(self.hparams.max_epochs*0.8)],\n",
        "                                                      gamma=0.1)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def _calculate_loss(self, batch, mode='train'):\n",
        "        feats, labels = batch\n",
        "        preds = self.model(feats)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        self.log(mode + '_loss', loss)\n",
        "        self.log(mode + '_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._calculate_loss(batch, mode='train')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='val')\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpqv_iuu_8RV"
      },
      "source": [
        "The data we use is the training and test set of STL10. The training contains 500 images per class, while the test set has 800 images per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfNz2LQntVtU"
      },
      "outputs": [],
      "source": [
        "# To create transformations, we needed a callebale class which would run the transformation\n",
        "# We have specfically designed such a class and returned the style transferred image value\n",
        "class StyleTransferTransform:\n",
        "    def __init__(self, style_path, epochs=10, content_weight=1e4, style_weight=1e-2):\n",
        "        self.style_path = style_path\n",
        "        self.epochs = epochs\n",
        "        self.content_weight = content_weight\n",
        "        self.style_weight = style_weight\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return run_style_transfer(\n",
        "            content_path=img,\n",
        "            style_path=self.style_path,\n",
        "            epochs=self.epochs,\n",
        "            content_weight=self.content_weight,\n",
        "            style_weight=self.style_weight\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXxbz4gm_8RV",
        "outputId": "34e9e553-cdda-4a97-ca87-e8926dbb261e"
      },
      "outputs": [],
      "source": [
        "# The style to use will be given in the report, it is pregiven to the object to use\n",
        "transform_style = StyleTransferTransform(style_path = 'style_3.jpg')\n",
        "\n",
        "# Getting the image data for train and test sets\n",
        "train_img_data = STL10(root = DATASET_PATH, split = 'train', download = True,\n",
        "                       transform = transform_style)\n",
        "test_img_data = STL10(root = DATASET_PATH, split = 'test', download = True,\n",
        "                      transform = transform_style)\n",
        "\n",
        "print(\"Number of training examples:\", len(train_img_data))\n",
        "print(\"Number of test examples:\", len(test_img_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_9gDihr_8RV"
      },
      "source": [
        "Next, we implement a small function to encode all images in our datasets. The output representations are then used as inputs to the Logistic Regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CNZgNjJ_8RV"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def prepare_data_features(model, dataset):\n",
        "    # Preparing the model\n",
        "    network = deepcopy(model.convnet)\n",
        "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
        "    network.eval()\n",
        "    network.to(device)\n",
        "\n",
        "    # Encoding all images\n",
        "    # Here, if we were to do multiprocessing we could have used num_workers not as 0\n",
        "    data_loader = data.DataLoader(dataset, batch_size = 10, num_workers = NUM_WORKERS, shuffle = False, drop_last = False)\n",
        "\n",
        "    feats, labels = [], []\n",
        "    for batch_imgs, batch_labels in tqdm(data_loader):\n",
        "        batch_imgs = batch_imgs.to(device)\n",
        "        batch_feats = network(batch_imgs)\n",
        "        feats.append(batch_feats.detach().cpu())\n",
        "        labels.append(batch_labels)\n",
        "\n",
        "    feats = torch.cat(feats, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "\n",
        "    labels, idxs = labels.sort()\n",
        "    feats = feats[idxs]\n",
        "\n",
        "    return data.TensorDataset(feats, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26wSVrzM_8RW"
      },
      "source": [
        "Let's apply the function to both training and test set below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "221bbde539314ad7bc3885f7564173bb",
            "6a503d0b9b4a4cad932afb9e921dae07",
            "91288f54b29e4326a6c571262db8ecf4",
            "19d878ef55774a6f9c901461710a68e9",
            "2d6658f6265247fab206c3853b895433",
            "d787801371764b788dc316a132acc6fd",
            "9ccdfdffb5454c58b68a73cd61d15c47",
            "11d21536ea24439ab55f225a7fa13df6",
            "d0bb2c970c454dcb935c9872c9470f22",
            "ebb6bc3269974c68866f7843f6e6a20c",
            "b05da9096b064e11a4103977b3c8b4c5",
            "7479d64067974964a9b9fefec77db501",
            "76ff0f783f9f4de68ed20db436527e9a",
            "23e0c7da1bb043d89f5e75b9127ebc54",
            "78d5d338462943689dea4c6f8e23d648",
            "852bfe8804254e4eb7f95df8d6346b7b",
            "8bce4a4b557d4111b53abb42e214d04d",
            "e755d0da1e9a48f0b6249f83b6b6b067",
            "33b7006be2c34ebfab2be293540df632",
            "5741662b6af347a5b8b643bfa4a95bf4",
            "473eda65888c4e56b6751eda43ff5b5a",
            "e58300022e4847b1b2231aa35bde0203"
          ]
        },
        "id": "dfnsjVLa_8RW",
        "outputId": "077bbe20-1e23-4804-8b81-e90725862618"
      },
      "outputs": [],
      "source": [
        "train_feats_simclr = prepare_data_features(simclr_model, train_img_data)\n",
        "test_feats_simclr = prepare_data_features(simclr_model, test_img_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zK05lTu2OiGa"
      },
      "outputs": [],
      "source": [
        "torch.save(test_feats_simclr.tensors, 'test_feats_simclr_10.pt')\n",
        "torch.save(train_feats_simclr.tensors, 'train_feats_simclr_10.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy3Wxija_8RW"
      },
      "source": [
        "Finally, we can write a training function as usual. We evaluate the model on the test set every 10 epochs to allow early stopping, but the low frequency of the validation ensures that we do not overfit too much on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6ztPyIjR_8RX"
      },
      "outputs": [],
      "source": [
        "def train_logreg(batch_size, train_feats_data, test_feats_data, model_suffix, max_epochs=100, **kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"),\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=max_epochs,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc'),\n",
        "                                    LearningRateMonitor(\"epoch\")],\n",
        "                         enable_progress_bar=False,\n",
        "                         check_val_every_n_epoch=10)\n",
        "    trainer.logger._default_hp_metric = None\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = data.DataLoader(train_feats_data, batch_size=batch_size, shuffle=True,\n",
        "                                   drop_last=False, pin_memory=True, num_workers=0)\n",
        "    test_loader = data.DataLoader(test_feats_data, batch_size=batch_size, shuffle=False,\n",
        "                                  drop_last=False, pin_memory=True, num_workers=0)\n",
        "\n",
        "    pl.seed_everything(42)  # To be reproducable\n",
        "    model = LogisticRegression(**kwargs)\n",
        "    trainer.fit(model, train_loader, test_loader)\n",
        "    model = LogisticRegression.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Test best model on train and validation set\n",
        "    train_result = trainer.test(model, train_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"train\": train_result[0][\"test_acc\"], \"test\": test_result[0][\"test_acc\"]}\n",
        "\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEfRDikl_8RX"
      },
      "source": [
        "Despite the training dataset of STL10 already only having 500 labeled images per class, we will perform experiments with even smaller datasets. Specifically, we train a Logistic Regression model for datasets with only 10, 20, 50, 100, 200, and all 500 examples per class. This gives us an intuition on how well the representations learned by contrastive learning can be transfered to a image recognition task like this classification. First, let's define a function to create the intended sub-datasets from the full training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5KSXbaTQ_8RX"
      },
      "outputs": [],
      "source": [
        "def get_smaller_dataset(original_dataset, num_imgs_per_label):\n",
        "    new_dataset = data.TensorDataset(\n",
        "        *[t.unflatten(0, (10, -1))[:,:num_imgs_per_label].flatten(0, 1) for t in original_dataset.tensors]\n",
        "    )\n",
        "    return new_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-G4TcUK_8RX"
      },
      "source": [
        "Next, let's run all models. Despite us training 6 models, this cell could be run within a minute or two without the pretrained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JT3eJiO5_8RY",
        "outputId": "f6c57afa-71bc-459f-d91a-d4a397eacce3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "for num_imgs_per_label in [10, 20, 50, 100, 200, 500]:\n",
        "    sub_train_set = get_smaller_dataset(train_feats_simclr, num_imgs_per_label)\n",
        "    _, small_set_results = train_logreg(batch_size=64,\n",
        "                                        train_feats_data=sub_train_set,\n",
        "                                        test_feats_data=test_feats_simclr,\n",
        "                                        model_suffix=num_imgs_per_label,\n",
        "                                        feature_dim=train_feats_simclr.tensors[0].shape[1],\n",
        "                                        num_classes=10,\n",
        "                                        lr=1e-3,\n",
        "                                        weight_decay=1e-3)\n",
        "    results[num_imgs_per_label] = small_set_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DyM4DGK_8RY"
      },
      "source": [
        "Finally, let's plot the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_fWL6LNV_8RY",
        "outputId": "70e537a7-5a6c-4aa4-dd9e-c753d5ec1647"
      },
      "outputs": [],
      "source": [
        "dataset_sizes = sorted([k for k in results])\n",
        "test_scores = [results[k][\"test\"] for k in dataset_sizes]\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.plot(dataset_sizes, test_scores, '--', color=\"#000\", marker=\"*\", markeredgecolor=\"#000\", markerfacecolor=\"y\", markersize=16)\n",
        "plt.xscale(\"log\")\n",
        "plt.xticks(dataset_sizes, labels=dataset_sizes)\n",
        "plt.title(\"STL10 classification over dataset size\", fontsize=14)\n",
        "plt.xlabel(\"Number of images per class\")\n",
        "plt.ylabel(\"Test accuracy\")\n",
        "plt.minorticks_off()\n",
        "plt.show()\n",
        "\n",
        "for k, score in zip(dataset_sizes, test_scores):\n",
        "    print(f'Test accuracy for {k:3d} images per label: {100*score:4.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hhXbOPP_8RY"
      },
      "source": [
        "## Baseline\n",
        "\n",
        "As a baseline to our results above, we will train a standard ResNet-18 with random initialization on the labeled training set of STL10. The results will HOPEFULLY give us an indication of the advantages that contrastive learning on unlabeled data has compared to using only supervised training. The implementation of the model is straightforward since the ResNet architecture is provided in the torchvision library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fOCs22nW_8RY"
      },
      "outputs": [],
      "source": [
        "class ResNet(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, num_classes, lr, weight_decay, max_epochs=100):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = torchvision.models.resnet18(num_classes=num_classes)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(),\n",
        "                                lr=self.hparams.lr,\n",
        "                                weight_decay=self.hparams.weight_decay)\n",
        "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                      milestones=[int(self.hparams.max_epochs*0.7),\n",
        "                                                                  int(self.hparams.max_epochs*0.9)],\n",
        "                                                      gamma=0.1)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def _calculate_loss(self, batch, mode='train'):\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        self.log(mode + '_loss', loss)\n",
        "        self.log(mode + '_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._calculate_loss(batch, mode='train')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='val')\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iwS3dVkB_8RZ",
        "outputId": "e51a0e03-3652-4b99-c693-4815ec861ac7"
      },
      "outputs": [],
      "source": [
        "transform_style = StyleTransferTransform(style_path='style_3.jpg')\n",
        "\n",
        "train_img_aug_data = STL10(root=DATASET_PATH, split='train', download=True,\n",
        "                           transform=transform_style)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqyD6zi1_8RZ"
      },
      "source": [
        "The training function for the ResNet is almost identical to the Logistic Regression setup. Note that we allow the ResNet to perform validation every 2 epochs to also check whether the model overfits strongly in the first iterations or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xNj7A1jJ_8RZ"
      },
      "outputs": [],
      "source": [
        "def train_resnet(batch_size, max_epochs=50, **kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ResNet\"),\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=max_epochs,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
        "                                    LearningRateMonitor(\"epoch\")],\n",
        "                         check_val_every_n_epoch=2)\n",
        "    trainer.logger._default_hp_metric = None\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = data.DataLoader(train_img_aug_data, batch_size=batch_size, shuffle=True,\n",
        "                                   drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "    test_loader = data.DataLoader(test_img_data, batch_size=batch_size, shuffle=False,\n",
        "                                  drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ResNet.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model at %s, loading...\" % pretrained_filename)\n",
        "        model = ResNet.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything(42) # To be reproducable\n",
        "        model = ResNet(**kwargs)\n",
        "        trainer.fit(model, train_loader, test_loader)\n",
        "        model = ResNet.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Test best model on validation set\n",
        "    train_result = trainer.test(model, train_loader, verbose=False)\n",
        "    val_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"train\": train_result[0][\"test_acc\"], \"test\": val_result[0][\"test_acc\"]}\n",
        "\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr05epz-_8RZ"
      },
      "source": [
        "Finally, let's train the model and check its results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQPeFEC4_8Ra"
      },
      "outputs": [],
      "source": [
        "resnet_model, resnet_result = train_resnet(batch_size=64,\n",
        "                                           num_classes=10,\n",
        "                                           lr=1e-3,\n",
        "                                           weight_decay=2e-4,\n",
        "                                           max_epochs=50)\n",
        "print(f\"Accuracy on training set: {100*resnet_result['train']:4.2f}%\")\n",
        "print(f\"Accuracy on test set: {100*resnet_result['test']:4.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11d21536ea24439ab55f225a7fa13df6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19d878ef55774a6f9c901461710a68e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebb6bc3269974c68866f7843f6e6a20c",
            "placeholder": "​",
            "style": "IPY_MODEL_b05da9096b064e11a4103977b3c8b4c5",
            "value": " 500/500 [1:31:58&lt;00:00, 11.02s/it]"
          }
        },
        "221bbde539314ad7bc3885f7564173bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a503d0b9b4a4cad932afb9e921dae07",
              "IPY_MODEL_91288f54b29e4326a6c571262db8ecf4",
              "IPY_MODEL_19d878ef55774a6f9c901461710a68e9"
            ],
            "layout": "IPY_MODEL_2d6658f6265247fab206c3853b895433"
          }
        },
        "23e0c7da1bb043d89f5e75b9127ebc54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b7006be2c34ebfab2be293540df632",
            "max": 800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5741662b6af347a5b8b643bfa4a95bf4",
            "value": 612
          }
        },
        "2d6658f6265247fab206c3853b895433": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33b7006be2c34ebfab2be293540df632": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "473eda65888c4e56b6751eda43ff5b5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5741662b6af347a5b8b643bfa4a95bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a503d0b9b4a4cad932afb9e921dae07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d787801371764b788dc316a132acc6fd",
            "placeholder": "​",
            "style": "IPY_MODEL_9ccdfdffb5454c58b68a73cd61d15c47",
            "value": "100%"
          }
        },
        "7479d64067974964a9b9fefec77db501": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76ff0f783f9f4de68ed20db436527e9a",
              "IPY_MODEL_23e0c7da1bb043d89f5e75b9127ebc54",
              "IPY_MODEL_78d5d338462943689dea4c6f8e23d648"
            ],
            "layout": "IPY_MODEL_852bfe8804254e4eb7f95df8d6346b7b"
          }
        },
        "76ff0f783f9f4de68ed20db436527e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bce4a4b557d4111b53abb42e214d04d",
            "placeholder": "​",
            "style": "IPY_MODEL_e755d0da1e9a48f0b6249f83b6b6b067",
            "value": " 76%"
          }
        },
        "78d5d338462943689dea4c6f8e23d648": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_473eda65888c4e56b6751eda43ff5b5a",
            "placeholder": "​",
            "style": "IPY_MODEL_e58300022e4847b1b2231aa35bde0203",
            "value": " 612/800 [1:52:17&lt;34:17, 10.94s/it]"
          }
        },
        "852bfe8804254e4eb7f95df8d6346b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bce4a4b557d4111b53abb42e214d04d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91288f54b29e4326a6c571262db8ecf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11d21536ea24439ab55f225a7fa13df6",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0bb2c970c454dcb935c9872c9470f22",
            "value": 500
          }
        },
        "9ccdfdffb5454c58b68a73cd61d15c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b05da9096b064e11a4103977b3c8b4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0bb2c970c454dcb935c9872c9470f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d787801371764b788dc316a132acc6fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e58300022e4847b1b2231aa35bde0203": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e755d0da1e9a48f0b6249f83b6b6b067": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebb6bc3269974c68866f7843f6e6a20c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
