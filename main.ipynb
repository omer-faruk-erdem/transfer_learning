{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing sys to check the current Python executable\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "# Import necessary libraries and install if they are not already installed\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    'numpy', 'pandas', 'opencv-python', 'Pillow', 'torch', 'torchvision',\n",
    "    'scikit-learn', 'jupyter'\n",
    "]\n",
    "\n",
    "# Install the required packages\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        install(package)\n",
    "\n",
    "# Now you can safely import the packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import time # for random seed\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained VGG model\n",
    "vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "# Load pre-trained ResNet18 model\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18 = torch.nn.Sequential(*(list(resnet18.children())[:-1])).to(device).eval()\n",
    "\n",
    "\n",
    "print(\"All packages are installed and imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the id-labels mapping in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny-imagenet-200 dataset\n",
    "data_dir = \"tiny-imagenet-200\"\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "\n",
    "\n",
    "# get all of the classes whose images are in the training set\n",
    "training_class_ids = os.listdir(train_dir)\n",
    "\n",
    "# class id - class label mapping\n",
    "id_class_label_map = {}\n",
    "with open(os.path.join(data_dir, 'words.txt'), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split('\\t')\n",
    "        class_id = line[0]\n",
    "        class_label = line[1]\n",
    "        id_class_label_map[class_id] = class_label\n",
    "\n",
    "\n",
    "# for each class in the tiny-imagenet-200 dataset, get the class label\n",
    "training_id_class_map = {}\n",
    "for class_id in training_class_ids:\n",
    "    class_label = id_class_label_map[class_id]\n",
    "    training_id_class_map[class_id] = class_label\n",
    "\n",
    "# we have 200 classes in the tiny-imagenet-200 dataset with their class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random 10 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ten_random_class_ids(training_class_ids):\n",
    "    # get 10 random classes\n",
    "\n",
    "    # seed random with time\n",
    "    np.random.seed(int(time.time()))\n",
    "\n",
    "    ten_random_classes = np.random.choice(training_class_ids, 10, replace=False)\n",
    "    return ten_random_classes\n",
    "\n",
    "\n",
    "print(\"10 random classes:\")\n",
    "ten_random_classes = get_ten_random_class_ids(training_class_ids)\n",
    "for i in range(10):\n",
    "    print(f\"{i+1}. {ten_random_classes[i]} - {training_id_class_map[ten_random_classes[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the images in the training set for each of the 10 random classes\n",
    "# class-id to class images\n",
    "training_class_images = {}\n",
    "for class_id in ten_random_classes:\n",
    "    class_images = os.listdir(os.path.join(train_dir, class_id, 'images'))\n",
    "    training_class_images[class_id] = class_images\n",
    "\n",
    "for class_id in ten_random_classes:\n",
    "    print(f\"{class_id} - {training_id_class_map[class_id]} has {len(training_class_images[class_id])} images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style-Transfer 50% of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained VGG model\n",
    "vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#Function to load and transform an image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "# Function to convert tensor back to image\n",
    "def tensor_to_image(tensor):\n",
    "    if tensor.dim() == 4:  # If tensor is of shape (1, C, H, W)\n",
    "        tensor = tensor.squeeze(0)\n",
    "    image = tensor.clone().detach().cpu().numpy()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    image = image.clip(0, 1)\n",
    "    return Image.fromarray((image * 255).astype('uint8'))\n",
    "\n",
    "    \n",
    "\n",
    "# Function to compute Gram matrix\n",
    "def gram_matrix(input):\n",
    "    batch_size, feature_maps, h, w = input.size()\n",
    "    features = input.view(batch_size * feature_maps, h * w)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(batch_size * feature_maps * h * w)\n",
    "\n",
    "# Define content and style layers\n",
    "content_layers = ['0']\n",
    "style_layers = ['0', '5', '10', '19', '28']\n",
    "\n",
    "# Function to extract features\n",
    "def get_features(image, model, layers):\n",
    "    features = {}\n",
    "    x = image\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[name] = x\n",
    "    return features\n",
    "\n",
    "\n",
    "def style_transfer(content_img, style_img, model, content_weight=1, style_weight=1e6, num_steps=300):\n",
    "    content_features = get_features(content_img, model, content_layers)\n",
    "    style_features = get_features(style_img, model, style_layers)\n",
    "    \n",
    "    target = content_img.clone().requires_grad_(True).to(device)\n",
    "    optimizer = torch.optim.Adam([target], lr=0.003)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        target_features = get_features(target, model, content_layers + style_layers)\n",
    "        \n",
    "        content_loss = F.mse_loss(target_features['0'], content_features['0'])\n",
    "        style_loss = 0\n",
    "        for layer in style_layers:\n",
    "            style_loss += F.mse_loss(gram_matrix(target_features[layer]), gram_matrix(style_features[layer]))\n",
    "        \n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward(retain_graph=True)  # Retain the graph\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}/{num_steps}, Content Loss: {content_loss.item()}, Style Loss: {style_loss.item()}\")\n",
    "    \n",
    "    return target\n",
    "\n",
    "\n",
    "output_dir = 'styled_images'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Process images for style transfer\n",
    "for class_id in ten_random_classes:\n",
    "    class_dir = os.path.join(train_dir, class_id, 'images')\n",
    "    image_names = training_class_images[class_id]\n",
    "    \n",
    "    # Randomly select 50% of the images for style transfer\n",
    "    selected_images = random.sample(image_names, len(image_names) // 2)\n",
    "    \n",
    "    for image_name in image_names:\n",
    "        img_path = os.path.join(class_dir, image_name)\n",
    "        content_image = load_image(img_path)\n",
    "        \n",
    "        if image_name in selected_images:\n",
    "            style_img_path = os.path.join(class_dir, random.choice(image_names))\n",
    "            style_image = load_image(style_img_path)\n",
    "            styled_image = style_transfer(content_image, style_image, vgg, num_steps=100)\n",
    "            output_image = tensor_to_image(styled_image)\n",
    "            output_image.save(os.path.join(output_dir, f\"{class_id}_styled_{image_name}\"))\n",
    "        else:\n",
    "            output_image = tensor_to_image(content_image.squeeze(0))\n",
    "            output_image.save(os.path.join(output_dir, f\"{class_id}_original_{image_name}\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Feature and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to extract features using ResNet18\n",
    "def extract_features(images, model):\n",
    "    features = []\n",
    "    for image in images:\n",
    "        with torch.no_grad():\n",
    "            feature = model(image.unsqueeze(0))  # Add batch dimension\n",
    "            feature = feature.view(feature.size(0), -1)  # Flatten features\n",
    "            features.append(feature.cpu().numpy())\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "\n",
    "# Extract features for all images and prepare dataset\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for class_id in ten_random_classes:\n",
    "    for image_name in training_class_images[class_id]:\n",
    "        if f\"{class_id}_styled_{image_name}\" in os.listdir(output_dir):\n",
    "            img_path = os.path.join(output_dir, f\"{class_id}_styled_{image_name}\")\n",
    "        else:\n",
    "            img_path = os.path.join(output_dir, f\"{class_id}_original_{image_name}\")\n",
    "        \n",
    "        image = load_image(img_path)\n",
    "        features = extract_features([image], resnet18)\n",
    "        X.append(features[0])\n",
    "        y.append(class_id)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression classifier\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X, y_encoded)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = classifier.predict(X)\n",
    "accuracy = accuracy_score(y_encoded, y_pred)\n",
    "precision = precision_score(y_encoded, y_pred, average='weighted')\n",
    "recall = recall_score(y_encoded, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
